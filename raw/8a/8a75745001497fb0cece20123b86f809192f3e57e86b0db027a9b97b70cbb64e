package demo;

import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

//                                             k3        v3       k4      v4
public class WordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable> {

	@Override
	protected void reduce(Text k3, Iterable<IntWritable> v3,Context context) throws IOException, InterruptedException {
		/*
		 * context 代表reduce的上下文
		 * 上：map
		 * 下：HDFS
		 * 
		 * 进行求和: 对同一个单词进行求和，对value3求和就能得到某个单词总的次数
		 * 很重要的原则：相同的key2 他的value2会被同一个Reduce处理
		 */
		int total = 0;
		for(IntWritable v:v3){
			total = v.get() + total;
		}
		
		//输出                                  k4单词   v4 总的次数
		context.write(k3, new IntWritable(total));
	}

}













